{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))       # Inception model used for FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_true -- true labels\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis = -1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis = -1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0),axis = None)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database of Authorized person \n",
    "\n",
    "database = {}     \n",
    "\n",
    "img2 = cv2.imread(\"sample_image.jpg\",1)                           # loading the image in databse (Image should be of dimension (96,96,3))\n",
    "database[\"AuthorizedPerson\"] = img_to_encoding(img2,FRmodel)      #Converting image into identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the authorized person\n",
    "# by calculating the distance between image of detected person and the authorized person\n",
    "\n",
    "def verify(img, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"img\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "    img -- image \n",
    "    identity -- string, name of the person you'd like to verify the identity. \n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image and the image of \"identity\" in the database.\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.resize(img, (96,96), 1)                         # Resize image for encoding\n",
    "    encoding = img_to_encoding(img,model)                     # Get the encoding vector of image\n",
    "    dist =  np.linalg.norm(database[identity] - encoding)     # Find the distance beetween the encoding of \n",
    "                                                              # image and encoding of authorized image's encoding\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Yolo\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")             # Loading yolo model \n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]            # Get name of classes for coco data sets\n",
    "layer_names = net.getLayerNames()                                 # List of layers in Yolo model\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOURCE : https://pysource.com/2019/07/08/yolo-real-time-detection-on-cpu/\n",
    "# Loading image\n",
    "#cap = cv2.VideoCapture(\"video.mp4\")\n",
    "cap = cv2.VideoCapture(0)                                                            # Capture video from webcam\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')                                             # For saving the output video\n",
    "save = cv2.VideoWriter('output.avi', fourcc, 20.0, (640,  360))                      # Name of the file to save and dimesnions\n",
    "font = cv2.FONT_HERSHEY_PLAIN                                                        # Font for text to be used in frame for detected objects\n",
    "starting_time = time.time()\n",
    "frame_id = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")                      # If no frames received loop will break\n",
    "        break\n",
    "    frame_id += 1\n",
    "\n",
    "    height, width, channels = frame.shape                                            # Get shape of frame\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)                                                # Passing the image to the model for object detection\n",
    "\n",
    "    # Showing informations on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.2:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.8, 0.3)                             # Applying non max supression for detecting object\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            \n",
    "            label = str(classes[class_ids[i]])\n",
    "            \n",
    "            confidence = confidences[i]\n",
    "            color = colors[class_ids[i]]\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            if label==\"person\":\n",
    "                frametoinput = frame\n",
    "                \n",
    "                # verify takes current frame, Identity of Authorized person, databse and modelinstance as input\n",
    "                \n",
    "                dist = verify(frametoinput, \"AuthorizedPerson\", database, FRmodel)         # Measure the norm distance between face in frame and \n",
    "                                                                                           # authorized person's face encoding\n",
    "                if dist<0.5:                                                               # If checking for confidence \n",
    "                    cv2.putText(frame, \"AuthorizedPerson\", (x, y + 30), font, 2, color, 2)        # Put text in the frame\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Unknown, Another person detected\",\\\n",
    "                                (x, y + 30), font, 2, color, 2)\n",
    "            if label!=\"person\":\n",
    "                cv2.putText(frame, label + \" detected\", (x+30, y+30), font, 2, color, 2)\n",
    "    \n",
    "    elapsed_time = time.time() - starting_time\n",
    "    fps = frame_id / elapsed_time                                                          # Calculating FPS\n",
    "    cv2.putText(frame, \"FPS: \" + str(round(fps, 2)), (10, 50), font, 2, (0, 0, 0), 3)\n",
    "    save.write(frame)                                                                      # Saving the output video\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 0:\n",
    "        break\n",
    "cap.release()\n",
    "save.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
